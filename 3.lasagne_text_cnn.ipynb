{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text CNN\n",
    "Chaining multiple convolutional outputs. <br>\n",
    "Yoon Kim, Convolutional Neural Networks for Sentence Classification,  EMNLP 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K40c (CNMeM is disabled, cuDNN 4007)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import theano \n",
    "import theano.tensor as T\n",
    "import lasagne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Model Parameters\n",
    "embedding_dim = 128 #Dimensionality of character embedding\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 2 #Number of filters per filter size\n",
    "dropout_keep_prob = 0.5 #Dropout keep probability\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 64\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data details\n",
    "IMDB dataset for binary sentiment classification. <br>\n",
    "Data has parsed so as map every distinct word to an integer. <br>\n",
    "And labels have been converted to one hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data parameters\n",
    "#maximum distinct words found in the corpus.\n",
    "vocab_len = 18758 \n",
    "#maximum sequence len under consideration. Padded with 0's.\n",
    "seq_len = 56 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample x: [ 1  2  3  4  5  6  1  7  8  9 10 11 12 13 14  9 15  5 16 17 18 19 20 21 22\n",
      " 23 24 25 26 27 28 29 30  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0]\n",
      ".................................\n",
      "sample y: [0 1]\n"
     ]
    }
   ],
   "source": [
    "x = np.load(\"./data/text_x.npy\").astype(np.int32)\n",
    "y = np.load(\"./data/text_y.npy\").astype(np.int32)\n",
    "\n",
    "print \"sample x:\", x[0]\n",
    "print \".................................\"\n",
    "print \"sample y:\", y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#shuffle data\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x = x[shuffle_indices]\n",
    "y = y[shuffle_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#split to training and validation sets\n",
    "x_train, x_val = x[:-1000], x[-1000:]\n",
    "y_train, y_val = y[:-1000], y[-1000:]\n",
    "\n",
    "#dont need original data. Free memory\n",
    "del x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_network(input_var, drop_prob):\n",
    "    \n",
    "    network = lasagne.layers.InputLayer(shape=(None, seq_len),\n",
    "                                        input_var=input_var,\n",
    "                                       name = \"input\")\n",
    "    \n",
    "    network = lasagne.layers.embedding.EmbeddingLayer(network,\n",
    "                                                       input_size = vocab_len, \n",
    "                                                       output_size = embedding_dim,\n",
    "                                                     name = \"embedding\")\n",
    "    \n",
    "    network = lasagne.layers.reshape(network,(-1,1,seq_len,embedding_dim),\n",
    "                                    name = \"reshape\")\n",
    "    \n",
    "    #observe how symbolics can be saved in list and symbolically concatenated!!\n",
    "    pooled_outputs = []\n",
    "    for i , f in enumerate(filter_sizes):\n",
    "        conv = lasagne.layers.conv.Conv2DLayer(network,num_filters,(f,f),\n",
    "                                               nonlinearity=lasagne.nonlinearities.rectify,\n",
    "                                              name = \"conv-\"+str(f))\n",
    "        \n",
    "        pool = lasagne.layers.pool.MaxPool2DLayer(conv, [seq_len - f + 1,1],\n",
    "                                           stride=[1, 1], name = \"pool-\"+str(f))\n",
    "        pooled_outputs.append(pool)\n",
    "    \n",
    "    \n",
    "    network = lasagne.layers.ConcatLayer(pooled_outputs,axis=3, name = \"concat\")\n",
    "    \n",
    "    num_filters_total = num_filters * len(filter_sizes)\n",
    "    \n",
    "    network = lasagne.layers.reshape(network,(-1,num_filters*375), name = \"reshape\")\n",
    "       \n",
    "    network = lasagne.layers.dropout(network,p=drop_prob, name = \"dropout\")\n",
    "    \n",
    "    network = lasagne.layers.DenseLayer(\n",
    "            network,\n",
    "            num_units=2,\n",
    "            nonlinearity=lasagne.nonlinearities.softmax,\n",
    "            name = \"dense\")\n",
    "\n",
    "    return  network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#regular symbolics\n",
    "ip = T.imatrix(\"inputs\")\n",
    "op = T.imatrix(\"outputs\")\n",
    "#see how drop_probablity can now be passed while training!!\n",
    "drop_prob = T.scalar(\"drop_prob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input (None, 56)\n",
      "embedding (None, 56, 128)\n",
      "reshape (None, 1, 56, 128)\n",
      "conv-3 (None, 2, 54, 126)\n",
      "pool-3 (None, 2, 1, 126)\n",
      "conv-4 (None, 2, 53, 125)\n",
      "pool-4 (None, 2, 1, 125)\n",
      "conv-5 (None, 2, 52, 124)\n",
      "pool-5 (None, 2, 1, 124)\n",
      "concat (None, 2, 1, 375)\n",
      "reshape (None, 750)\n",
      "dropout (None, 750)\n",
      "dense (None, 2)\n"
     ]
    }
   ],
   "source": [
    "network = build_network(ip,drop_prob)\n",
    "\n",
    "for layer in lasagne.layers.helper.get_all_layers(network):\n",
    "    print layer.name, layer.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction = lasagne.layers.get_output(network)\n",
    "\n",
    "loss = lasagne.objectives.categorical_crossentropy(prediction, op)\n",
    "loss = loss.mean()\n",
    "params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "\n",
    "updates = lasagne.updates.adam(loss,params, learning_rate=1e-3)\n",
    "\n",
    "#observe drop_out becomes a parameter!!\n",
    "train_fn = theano.function([ip, op, drop_prob], \n",
    "                           loss, \n",
    "                           updates=updates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after epoch 1 training loss:  0.67266450727\n",
      "after epoch 2 training loss:  0.476986026891\n",
      "after epoch 3 training loss:  0.237351990477\n",
      "after epoch 4 training loss:  0.093088998301\n",
      "after epoch 5 training loss:  0.0360921119588\n",
      "after epoch 6 training loss:  0.0179557619787\n",
      "after epoch 7 training loss:  0.00885628417755\n",
      "after epoch 8 training loss:  0.00545781437618\n",
      "after epoch 9 training loss:  0.00398699526985\n",
      "after epoch 10 training loss:  0.0029546106793\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    train_loss = []\n",
    "    for i in range(0,x_train.shape[0]- batch_size, batch_size):\n",
    "        train_loss.append(\n",
    "            train_fn(\n",
    "                x_train[i:i+batch_size],\n",
    "                y_train[i:i+batch_size], \n",
    "                0.5\n",
    "            )\n",
    "        )\n",
    "    print \"after epoch\", epoch+1, \"training loss: \" , np.mean(train_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
